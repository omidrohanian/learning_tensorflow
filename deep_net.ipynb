{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to experiment with the MNIST dataset for recognizing handwritten digits using a feed-forward neural networks classifier. There is an input layer, some hidden layers and one final output layer.\n",
    "\n",
    "We compare the output of the network with the intended output using a cost function (cross entropy) and try to minimize the cost with an optimizer (AdamOptimizer, SGD, AdaGrad...). This requires going backwards to manipulate the weights (backpropagation). \n",
    "\n",
    "Each cycle of feed-forward + backpropagation is called an <i>epoch</i>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Reference:\n",
    "# https://www.youtube.com/watch?list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v&v=PwAGxqrXSCs\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data \n",
    "\n",
    "mnist = input_data.read_data_sets(\"tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defining the model \n",
    "# specifying the number of nodes for hidden layers (these could change)\n",
    "\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "\n",
    "# 10 classes: 0 - 9\n",
    "# each digit represented with a 10-dimentional list with one non-zero value\n",
    "# e.g. 4 = [0,0,0,1,0,0,0,0,0,0]\n",
    "\n",
    "n_classes = 10 \n",
    "\n",
    "# repeatedly go through batches of 100 of features and feed them through our network at a time \n",
    "# and manipulate the weights \n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# placeholders for the input and output matrix: \n",
    "# input matrix size: height x width = 28 * 28 = 784 \n",
    "\n",
    "x = tf.placeholder('float', [None, 784])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "def neural_network_model(data):\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([784, n_nodes_hl1])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                    'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    # (input_data * weights) + biases \n",
    "    \n",
    "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    output = tf.matmul(l3,output_layer['weights']) + output_layer['biases']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 completed out of 10 . loss: 1878542.74203\n",
      "epoch 1 completed out of 10 . loss: 409449.558212\n",
      "epoch 2 completed out of 10 . loss: 220113.94924\n",
      "epoch 3 completed out of 10 . loss: 131067.236988\n",
      "epoch 4 completed out of 10 . loss: 80119.2542363\n",
      "epoch 5 completed out of 10 . loss: 51459.1476424\n",
      "epoch 6 completed out of 10 . loss: 34180.2244798\n",
      "epoch 7 completed out of 10 . loss: 24406.4477171\n",
      "epoch 8 completed out of 10 . loss: 20571.1951414\n",
      "epoch 9 completed out of 10 . loss: 17014.7164322\n",
      "Accuracy: 0.9484\n"
     ]
    }
   ],
   "source": [
    "# the softmax function is a generalization of the logistic function that \"squashes\" a K-dimensional \n",
    "# vector of arbitrary real values to a K-dimensional vector of real values in the range (0, 1) that add up to 1\n",
    "\n",
    "# tensor: A typed multi-dimensional array\n",
    "\n",
    "# tf.reduce_mean: computes the mean of elements across dimensions of a tensor\n",
    "\n",
    "def train_neural_network(x):\n",
    "    # run the model first\n",
    "    prediction = neural_network_model(x)\n",
    "    # calculate the cost\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\n",
    "    # we'd like to minimize this cost. In this case we use the AdamOptimizer which is a \n",
    "    # a Stochastic gradient-based method for optimization. Its default learning rate is set \n",
    "    # to be 0.001 which is fine enough for our purposes here\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    # how many epochs do we need (we try with 10 initilly)\n",
    "    hm_epochs = 10\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0 \n",
    "            for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "                # c: cost\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                epoch_loss += c\n",
    "            print('epoch', epoch, 'completed out of', hm_epochs,'. loss:', epoch_loss)\n",
    "            \n",
    "        # Now that we have trained the model we can gauge its performance \n",
    "        # tf.argmax: returns the index with the largest value across axes of a tensor.\n",
    "        # we are checking to see if that value is the same in the intended output vs prediction \n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        # tf.cast: casts a tensor to a new type.\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy:', accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))\n",
    "        \n",
    "# Now we can actually run the netwrok\n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
